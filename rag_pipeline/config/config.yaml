# RAG Pipeline Configuration

# Path Configuration
paths:
  corpus_root: "data/corpus/scikit-learn-1.7.2-docs"
  processed_dir: "data/processed"
  state_dir: "data/state"
  logs_dir: "logs"
  vector_store_dir: "data/vector_store"

# Preprocessing Settings
preprocessing:
  # Input directory name pattern - automatically finds directories matching:
  # "scikit-learn-docs" (no version) or "scikit-learn-X.Y.Z-docs" (with version)
  input_name: "scikit-learn*-docs"  # Pattern for auto-detection
  force_reprocess: false  # Set to true to force re-pruning and re-processing
  file_patterns:
    - "*.html"
  # Folders to process within the pruned corpus
  process_folders:
    - "api"
    - "modules"
    - "auto_examples"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console_output: true
  file_output: true

# Chunking Configuration (Iteration 2)
chunking:
  strategies:
    fixed:
      enabled: true
      chunk_size: 512  # in tokens (~384 words)
      overlap: 50  # in tokens (~38 words)
    semantic:
      enabled: true
      max_chunk_size: 1000  # in words
      method: "sentence"  # "sentence" or "paragraph"
    hierarchical:
      enabled: true
      max_chunk_size: 1000  # in words

# Embedding Configuration (Iteration 2)
embeddings:
  model: "all-MiniLM-L6-v2"  # sentence-transformers model
  batch_size: 32
  device: "cpu"  # or "cuda" if GPU available
  
# Indexing Configuration (Iteration 2)
indexing:
  force_reindex: false  # Set to true to force re-indexing

# Retrieval Configuration (Iteration 3)
retrieval:
  vector_db: "chroma"
  top_k: 20  # Default number of results to return
  min_similarity: 0.0  # Minimum similarity score (0-1)
  strategy: "fixed"  # Default strategy: "fixed", "semantic", or "hierarchical"
  result_format: "detailed"  # "detailed" or "compact"
  
  # Hybrid search settings (BM25 + Semantic with RRF)
  search_mode: "hybrid"  # "semantic", "keyword", or "hybrid"
  hybrid_alpha: 0.7  # Weight for semantic search (0.0-1.0). 0.7 = 70% semantic, 30% keyword
  rrf_k: 60  # RRF constant. Higher = less emphasis on top ranks
  overfetch_factor: 3  # Retrieve this many times top_k before fusion

# Query Rewriting Configuration
query_rewriting:
  enabled: true  # Enable LLM-based query rewriting
  temperature: 0.3  # Low temperature for deterministic rewrites
  max_tokens: 100  # Maximum tokens for rewritten query
  timeout: 30  # Timeout for rewrite LLM call (seconds)
  cache_size: 128  # Maximum number of cached query rewrites

# Reranking Configuration
# Cross-encoder reranking improves retrieval quality by jointly scoring
# query-document pairs. Over-fetches documents, reranks, returns top results.
# Note: First reranking call may be slower due to model loading.
reranking:
  enabled: true
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # HuggingFace cross-encoder model
  overfetch_k: 50  # Number of documents to retrieve before reranking
  final_top_k: 10  # Number of documents to return after reranking (overrides retrieval.top_k)
  batch_size: 32  # Batch size for cross-encoder scoring
  device: "cpu"  # Device for cross-encoder ("cpu" or "cuda")

# Generation Configuration (Iteration 4)
generation:
  # Ollama settings
  ollama_base_url: "http://ollama:11434"  # Ollama service in Docker Compose
  model: "llama3.2:3b"  # Model name (llama3.2:3b, llama3.1:8b, mistral, etc.)
  timeout: 180  # Request timeout in seconds (increased for CPU generation)
  
  # Generation parameters
  temperature: 0.3  # Sampling temperature (0.0-1.0)
  max_tokens: 512  # Maximum tokens to generate (reduced for faster generation)
  top_p: 0.85  # Nucleus sampling parameter
  
  # Prompt settings
  max_context_length: 2000  # Maximum characters for context in prompt (reduced for speed)
  prompt_template: "default"  # "default" or "structured"
  include_sources: true  # Include sources section in answer

# Evaluation Configuration (Iteration 5)
evaluation:
  test_set_path: "data/evaluation/test_set.json"
  results_dir: "data/evaluation/results"
  
  # Strategies to evaluate
  strategies:
    - "fixed"
    - "semantic"
    - "hierarchical"
  
  # Metrics configuration
  top_k_values:
    - 5
    - 10
    - 20
  
  # Answer quality judging
  judge_answers: false  # Disabled: Llama 3.2 3B lacks discriminative capability for reliable judging
  judge_criteria:
    - "faithfulness"
    - "relevance"
    - "completeness"
    - "overall"

